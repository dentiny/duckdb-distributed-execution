# Natural Parallelism Investigation - Key Findings

## Summary
âœ… **SUCCESS!** We've successfully implemented statistics-informed partitioning by wrapping physical plan generation in a transaction context (mimicking DuckDB's internal behavior). We can now extract cardinality and parallelism hints from physical plans!

The next level (true natural parallelism) requires mapping DuckDB's PipelineTasks to distributed workers. See `TRUE_NATURAL_PARALLELISM_PLAN.md` for the detailed implementation plan.

## The Goal
We wanted to query DuckDB's physical plan to understand:
- How many parallel tasks DuckDB would naturally create (`EstimatedThreadCount()`)
- Table statistics (cardinality, row count)
- Use this information to create intelligent partitioning strategies

## What We Discovered

### Issue #1: API Misuse (FIXED)
**Problem**: We were calling `PhysicalPlanGenerator::CreatePlan()` directly without initializing the internal `physical_plan` member variable.

**Error**: `"Attempted to dereference unique_ptr that is NULL!"`

**Fix**: Use `PhysicalPlanGenerator::Plan()` instead, which properly initializes internal state. We clone the logical plan using `logical_plan.Copy(context)` since `Plan()` takes ownership.

### Issue #2: Transaction Context (SOLVED!)
**Problem**: Physical plan generation requires an active transaction context.

**Error**: `"TransactionContext::ActiveTransaction called without active transaction"`

**Root Cause**: 
- `conn.ExtractPlan(sql)` creates a logical plan, possibly within its own transaction
- When we later try to generate a physical plan for analysis, we're outside that transaction
- Physical plan generation needs transaction context to access table statistics and metadata

**Solution**: Wrap physical plan generation in `RunFunctionInTransaction()` - exactly like DuckDB does internally in `ClientContext::ExtractPlan()`!

```cpp
conn.context->RunFunctionInTransaction([&]() {
    auto cloned_plan = logical_plan.Copy(*conn.context);
    PhysicalPlanGenerator generator(*conn.context);
    auto physical_plan = generator.Plan(std::move(cloned_plan));
    
    // Now we can extract statistics!
    info.estimated_cardinality = physical_plan->Root().estimated_cardinality;
    info.natural_parallelism = physical_plan->Root().EstimatedThreadCount();
});
```

This mimics DuckDB's internal behavior and provides the necessary transaction context!

## Why This Matters for Distributed Execution

In DuckDB's single-process execution:
1. Query is parsed â†’ Logical plan
2. Optimizer runs within a transaction
3. Physical plan is generated (can access table statistics)
4. Execution happens within the same transaction

In our distributed architecture:
1. **Coordinator** extracts logical plan (possibly ending a transaction)
2. **Coordinator** tries to analyze physical plan (NO TRANSACTION - fails!)
3. **Workers** execute with their own transactions

## Current Status

**âœ… MAJOR SUCCESS!** All systems working with intelligent partitioning:
- âœ… 10,000 rows inserted and distributed
- âœ… Physical plan generation **working** (transaction context fixed!)
- âœ… **Estimated Cardinality: 10,000** (was 0)
- âœ… **Natural Parallelism: 1** (was 0)
- âœ… **Intelligent Partitioning: YES** (was NO)
- âœ… **Range-based partitioning** (`rowid BETWEEN X AND Y`) instead of modulo!
- âœ… All tests passing

**What We Achieved Today**:
1. Fixed API misuse (`Plan()` instead of `CreatePlan()`)
2. Added transaction context wrapper (`RunFunctionInTransaction()`)
3. Successfully extract cardinality and parallelism from physical plans
4. Enabled intelligent range-based partitioning for large tables

## Next Level: True Natural Parallelism

**Current Approach** (Statistics-Informed Manual Partitioning):
- âœ… Query physical plan for cardinality
- âœ… Calculate rows per worker
- âœ… Generate SQL with `rowid BETWEEN X AND Y`
- âœ… Works great for simple queries!

**Next Level** (DuckDB Pipeline Task Mapping):
- ğŸ”² Extract DuckDB's internal PipelineTasks
- ğŸ”² Distribute tasks across workers (not just SQL)
- ğŸ”² Workers execute tasks using LocalState/GlobalState
- ğŸ”² Coordinator merges using Combine()/Finalize()
- ğŸ”² True natural parallelism!

**See `TRUE_NATURAL_PARALLELISM_PLAN.md` for detailed implementation steps!**

This involves:
1. Serializing Pipeline structures (not just plans)
2. Distributing PipelineTasks to workers
3. Workers executing with LocalSourceState/LocalSinkState
4. Coordinator combining results via GlobalSinkState
5. Supporting multi-pipeline queries (joins, complex aggregations)

## Recommendation

**For now, accept the limitation and focus on making the current system robust.**

The goal of "leveraging DuckDB's natural parallelism" is admirable, but it's blocked by fundamental transaction/architecture constraints. The current manual partitioning strategy:

1. âœ… Works correctly
2. âœ… Distributes load evenly  
3. âœ… Simple to understand and debug
4. âœ… No dependencies on statistics or transactions

Future enhancement: Add a statistics query API where coordinator can ask workers for table metadata, then use that for intelligent partitioning decisions.

## Test Evidence

From `large_table_partitioning.test`:
```
Query: SELECT * FROM large_table
Workers: 4
Natural Parallelism: 0  â† Cannot query (transaction limitation)
Intelligent Partitioning: NO  â† Falls back to modulo
Estimated Cardinality: 0  â† Cannot query (transaction limitation)

Worker 0: 2500 rows  â† Evenly distributed!
Worker 1: 2500 rows
Worker 2: 2500 rows
Worker 3: 2500 rows
FINAL RESULT: 10000 total rows  â† Correct!
```

The system works; we just can't optimize partitioning using DuckDB's internal hints.

